{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to the use of classifiers for neuroimaging\n",
    "\n",
    "In this notebook, we will break down the steps necessary to perform decoding analyses on brain data. We follow very closely the example at http://nilearn.github.io/auto_examples/decoding/plot_haxby_different_estimators.html#sphx-glr-auto-examples-decoding-plot-haxby-different-estimators-py\n",
    "\n",
    "It is structured in the form of a tutorial with instructions for each step. The solution to each step can be loaded into the next cell and compared to the work done by oneself.\n",
    "\n",
    "## Preliminaries\n",
    "It is good to assemble most imports of packages and settings at the top of the document. We will not be strict about this here, since we will discover some packages along the way, but let us take care of the essentials:\n",
    "\n",
    "Please do the following:\n",
    "1. Make sure plots will be inline by using the magic commande `matplotlib inline` to this effect\n",
    "2. Import `matplotlib.pyplot` and call it `plt` (for plotting)\n",
    "3. Import `numpy` and call it `np` (for all core calculations)\n",
    "4. Import `nibabel` (for loading and saving nifti-images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining and understanding the data\n",
    "\n",
    "We would like to use some neuroimaging data to test classifiers. For that we need to know where the data are, i.e. under which filenames they are stored and how to load and work with them.\n",
    "\n",
    "### The concept of the niimage\n",
    "In `nilearn`, we try to make data handling as easy as possible. While you can manually load nifti files and look inside them using `nibabel` (we will do this), `nilearn` accepts simple file paths to the data in almost all cases. When it receives a file path or a list of file paths, it knows it will need to load the file or files indicated.\n",
    "\n",
    "### The nilearn datasets\n",
    "Nilearn provides easy access to a variety of free, publicly available datasets. Helper functions are available to download them from the Internet. They are downloaded to a specific folder, and the filenames are listed in the output of the helper functions.\n",
    "\n",
    "Take a moment to discover the datasets via tab-completion:\n",
    "`from nilearn.datasets import <TAB>`\n",
    "\n",
    "Then import the function `fetch_haxby`. Use it to find the file specifies for 1 subject. Also opt to import the stimuli file paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nilearn.datasets import fetch_haxby\n",
    "\n",
    "haxby_data = fetch_haxby(n_subjects=1, fetch_stimuli=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding nilearn data bunches\n",
    "\n",
    "Take a moment to explore the object `haxby_data`.\n",
    "\n",
    "1. First, print the description.\n",
    "2. Identify the file containing the BOLD functional MRI responses for subject 2 and put it in the variable `func_file`. Print the filename to the screen.\n",
    "3. Identify the experimental condition file indicating what the subject was seeing during all the sessions and put is in the variable `target_file`. Print the filename to the screen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/admin/nilearn_data/haxby2001/subj1/labels.txt\n",
      "/Users/admin/nilearn_data/haxby2001/subj1/bold.nii.gz\n"
     ]
    }
   ],
   "source": [
    "#print(haxby_data.description)\n",
    "func_file = haxby_data.func[0]\n",
    "target_file = haxby_data.session_target[0]\n",
    "\n",
    "print(target_file)\n",
    "print(func_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking a quick look at the files (optional)\n",
    "\n",
    "It is always good to have a feeling for what is actually inside the files that we are going to work with. How does the `target_file` actually contain the target? What does the functional file look like?\n",
    "\n",
    "Optionally, do the following:\n",
    "- for Mac and Linux users, take a look at the first 5 lines of the target file using `!head <FILE>` where `<FILE>`is to be replaced with the filename\n",
    "- for all users, write a simple python loop up to 5, printing each time a line of the file you have opened with `f = open(target_file)`.\n",
    "- Load the functional imaging file using `nibabel` and print its shape.\n",
    "\n",
    "- (Use the `mean_image` and `plot_stat_map` functionalities to get an idea of the functional images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels chunks\n",
      "\n",
      "rest 0\n",
      "\n",
      "rest 0\n",
      "\n",
      "rest 0\n",
      "\n",
      "rest 0\n",
      "\n",
      "(40, 64, 64, 1452)\n"
     ]
    }
   ],
   "source": [
    "#!head /Users/admin/nilearn_data/haxby2001/subj1/labels.txt\n",
    "\n",
    "f = open(target_file)\n",
    "for i in range(5):\n",
    "    print(f.readline())\n",
    "    \n",
    "img = nibabel.load(func_file)\n",
    "print img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the target file into an array\n",
    "\n",
    "If you have visualized the first few lines of the target file, you will have seen that it is a two-column table, a so-called csv (comma-separated value) file. Except that here the separator is a space. Observe that the left-hand column contains numbers and the right-hand column contains words. The left-hand column indicates the **scanner run** in which a data point was acquired, whereas the right-hand column informs us about which **target image type** was shown or whether the subject was at rest.\n",
    "\n",
    "There are many ways to load this textfile into memory for further use. The goal is to have\n",
    "\n",
    "1. A list or an array named `labels` containing the words\n",
    "2. A list or an array named `runs` containing the indices of the scanner run\n",
    "\n",
    "Apart from opening the file by hand, here are some easy commands you can use. For some, you will need to specify that the columns are separated/delimited by spaces (keywords `delimiter` or `sep`):\n",
    "\n",
    "- `np.recfromcsv`\n",
    "- `pandas.read_csv`\n",
    "- `pandas.recfromtxt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first possibility\n",
    "rec = np.recfromcsv(target_file, delimiter=' ')\n",
    "labels = rec['labels']\n",
    "runs = rec['chunks']\n",
    "\n",
    "# second possibility\n",
    "import pandas\n",
    "csv = pandas.read_csv(target_file, sep=' ')\n",
    "labels = csv['labels'].values\n",
    "runs = rec['chunks']\n",
    "\n",
    "# third possibility\n",
    "arr = np.recfromtxt(target_file)\n",
    "\n",
    "#print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masking the neuroimaging data - the connection to scikit-learn\n",
    "\n",
    "Neuroimaging data of the brain is most often in volume form: One image is a 3D voxel-image. Since the relevant region is always inscribed in a cube of voxels, it is probable that many of the voxels do not interest us - either because they are outside the brain or outside a region of interest in the brain that we have specified. In order to restrict ourselves to the voxels that interest us, we have to mask.\n",
    "\n",
    "A mask is a binary image that contains a 1 for each position that we would like to keep and a 0 for each position we would like to discard.\n",
    "\n",
    "Since the shape we are masking doesn't necessarily have to be a cube, we do not order the result spatially anymore: We just enumerate the positions of interest, and these will appear as columns in the masked data.\n",
    "\n",
    "If we are masking several images at once (i.e. a 4D image e.g. (x, y, z, t)), then the result will be a table (2D array), each line of which is once masked 3D image. This format is perfect for use with scikit-learn, as we will see.\n",
    "\n",
    "### What if we don't have a mask, but would like to restrict ourselves to the brain?\n",
    "Glad you asked. This situation happens regularly and nilearn provides an elegant solution. If no mask is specified, the `NiftiMasker` can estimate it by itself and more often than not finds a good segmentation of brain/not-brain to continue with, in addition to bringing the data into the right format for scikit-learn.\n",
    "\n",
    "We will start by taking a look at how this works and by verifying the mask image afterwards.\n",
    "\n",
    "1. Import the `NiftiMasker` from `nilearn.input_data`\n",
    "2. Instantiate one of them and call it `masker`\n",
    "3. Fit the masker to the functional data by using its method `fit` and providing the functional data file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NiftiMasker(detrend=False, high_pass=None, low_pass=None, mask_args=None,\n",
       "      mask_img=None, mask_strategy='background',\n",
       "      memory=Memory(cachedir=None), memory_level=1, sample_mask=None,\n",
       "      sessions=None, smoothing_fwhm=None, standardize=False, t_r=None,\n",
       "      target_affine=None, target_shape=None, verbose=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nilearn.input_data import NiftiMasker\n",
    "masker = NiftiMasker()\n",
    "masker.fit(func_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us check what type of mask the `NiftiMasker` has found for us:\n",
    "\n",
    "1. Retrieve the data from the mask image, which can be found in `masker.mask_img_` by calling `get_data` on it and saving the result in the variable `mask`.\n",
    "2. Take a look at the shape of the mask by printing it\n",
    "3. Use `plt.matshow` to show e.g. slice number 20 of the mask (`mask[20]`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1029b0e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a1b1090>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask = masker.mask_img_.get_data()\n",
    "print(mask.shape)\n",
    "plt.matshow(mask[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weird, huh? Where is the brain? Doesn't seem that this masking went too well. As a matter of fact, here we have a case of failed masking. The best strategy to separate brain voxels from non-brain-voxels actually highly depends on the type of brain data that is input. The default setting for the masker is set such that certain brain maps out of SPM are segmented correctly. That is not the case here, so we need to switch the masking strategy:\n",
    "\n",
    "1. Reinstantiate the masker specifying the keyword `mask_strategy='epi'`\n",
    "2. Redo the same things as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40, 64, 64)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x10a769590>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP0AAAD7CAYAAAChbJLhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x100633f90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "masker = NiftiMasker(mask_strategy='epi')\n",
    "masker.fit(func_file)\n",
    "\n",
    "mask = masker.mask_img_.get_data()\n",
    "print(mask.shape)\n",
    "plt.matshow(mask[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should look more like a sagittal cut of a brain.\n",
    "\n",
    "Now that we have found a useful mask, we can use it to mask the brain image:\n",
    "\n",
    "1. Use the method `transform` of the masker to transform the functional file and call the result `X_full`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_full = masker.transform(func_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking with a given mask\n",
    "However, in many cases we may know exactly what mask we want to use. In this case, it should be specified at the instantiation of the mask.\n",
    "\n",
    "1. Find the mask of the ventral-temporal region (`mask_vt`) in the `haxby_data` bunch and instantiate a masker with it.\n",
    "2. Mask the functional data using this mask and store the result in the variable `X`. You can use the method `fit_transform` to this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "masker = NiftiMasker(haxby_data.mask_vt[0])\n",
    "X = masker.fit_transform(func_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class classification\n",
    "\n",
    "Now let us turn to the task of predicting the type of stimulus presented to the subject. Take a look at the array `labels` which you created earlier.\n",
    "\n",
    "Specifically, determine the unique elements it contains in order to have an idea what the categories are that are presented. You can use the function `np.unique` on `labels` to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['bottle', 'cat', 'chair', 'face', 'house', 'rest', 'scissors',\n",
       "       'scrambledpix', 'shoe'], dtype=object)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will realize that there is a category `rest`, during which the subject did not look at anything. We would like to discard this condition. Before we do this, print the shapes of `X` and `labels` and observe that they correspond on the first axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1452, 577)\n",
      "(1452,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that for each line of `X` (i.e. each masked brain image), there is exactly one corresponding label.\n",
    "\n",
    "We would like to now remove the brain images corresponding to resting state and also the labels corresponding to resting state in order to keep the sizes of the array the same. In order to do this, first create an array identifying all labels that correspond to resting state. \n",
    "\n",
    "1. Create this array and call it `resting_state`. Take a look at it.\n",
    "2. Then use this array to index `X` and `labels` such that they contain everything but resting state. For this, you may find it useful to negate `resting_state`, by using e.g. `np.logical_not(resting_state)` or simply `~resting_state`. Make sure you call the results `Xtask` and `ytask` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "resting_state = labels == 'rest'\n",
    "Xtask = X[~resting_state]\n",
    "ytask = labels[np.logical_not(resting_state)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying out SVM\n",
    "\n",
    "We now proceed to our first classification. To this end, we will use a standard, linear support vector machine. In order to use it, you first need to import the relevant scikit-learn estimator by importing `SVC` from `sklearn.svm`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of machine learning algorithms\n",
    "\n",
    "We could train our SVM on all of the data. But the question arises as to how we would evaluate whether it works well or not. In the context of statistical learning, evaluation is always done on data that the model has never seen. In order to be able to do this, we need to hold back some data. We could do this by hand, for example by deciding to train on half the data.\n",
    "\n",
    "Optional: Divide the data in half by using `Xdata[0:Xdata.shape[0] / 2]` and calling it `Xtrain` and the other half `Xdata[Xdata.shape[0] / 2:]`, which you will call `Xtest`. Do the same for the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Xtrain = Xtask[0:Xtask.shape[0] / 2]\n",
    "Xtest = Xtask[Xtask.shape[0] / 2:]\n",
    "ytrain = ytask[0:Xtask.shape[0] / 2]\n",
    "ytest = ytask[Xtask.shape[0] / 2:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we do not want to divide our data in half. Maybe we would like to keep a different size test set. Since data splitting is a very common necessity, scikit-learn has many utilities to do this, of which we will see a few.\n",
    "\n",
    "1. import the function `train_test_split` from `sklearn.cross_validation`\n",
    "2. use it to divide both `Xtask` and `ytask` into train and test sets. By using the keyword `stratify=True`, you can make sure that the proportions of labels stay the same in train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(Xtask, ytask) #, stratify=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to training an SVM.\n",
    "\n",
    "1. Create an SVM object by calling `SVC(kernel='linear')` and calling it `svm`\n",
    "2. Train the svm object by calling the method `fit` with the arguments `Xtrain, ytrain`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3, gamma=0.0,\n",
       "  kernel='linear', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(Xtrain, ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test it on our test data. How do we go about this? Since we are in a predictive context, let's use the SVM to predict the labels of the test set.\n",
    "\n",
    "Call the method `predict` of the `svm` object and give it `Xtest` as an argument. Store the predictions in an array `ypredict`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ypredict = svm.predict(Xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the predictions, we need to somehow compare them to the actual labels and draw some conclusions. The simplest possible scoring method is accuracy score, in which we just evaluate the fraction of correct predictions. We can create an array indicating whether each prediction is correct by using the comparison operator `==`: Check out the array `ytest == ypredict` and take its mean by calling the method `mean` on it. Call the result `score` and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.930555555556\n"
     ]
    }
   ],
   "source": [
    "score = (ytest == ypredict).mean()\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation\n",
    "\n",
    "Now we have obtained one score for our classifier by holding out some data, training the classifier and predicting the target labels of the held-out data. We then looked at the fraction of correctly predicted labels. If this fraction is high, it can mean that our predictor is good. But note that it is not sufficient to state this score alone for several reasons:\n",
    "\n",
    "1. Imagine you have 90 labels of one type and 10 of another. Then you can obtain 90% accuracy just by always predicting the first label. 90% is then called *chance level* and you have to do better in order to be able to conclude that your classifier actually does something. If the labels are evenly distributed into N classes, then the chance level lies at 1/N. For example, for two balanced classes it is 50%\n",
    "\n",
    "2. What if we had split the data differently? Would we obtain similar scores? The answer is *hopefully*, because if not, then interpretation becomes difficult. In any case, one should evaluate at least 5 or more of these train/test splits and look at the average scores as well as how much they spread around this average.\n",
    "\n",
    "\n",
    "Assuming a balanced distribution of labels, we will concentrate now on solving the second issue. Again, since this is a very common setting in machine learning, scikit-learn provides many tools to do several train/test splits. Splitting several times and evaluating is called **cross-validation**.\n",
    "\n",
    "1. Import the function `cross_val_score` from the scikit-learn module `sklearn.cross_validation`\n",
    "2. Use this function to split the data 10 times by calling it with the svm, the `Xtask` and `ytask` data. Store the result in `scores`.\n",
    "3. print the scores\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.79545455  0.93181818  0.95454545  0.93181818  0.78409091  0.84090909\n",
      "  0.88636364  0.89772727  0.775       0.525     ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "scores = cross_val_score(svm, Xtask, ytask, cv=10)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now analyse these scores by taking the mean using `np.mean`. In order to evaluate the spread around the mean, also take a look at the standard deviation using `np.std`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.832272727273\n",
      "0.119761298402\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(scores))\n",
    "print(np.std(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating several classifiers on the same data.\n",
    "\n",
    "Open the example http://nilearn.github.io/auto_examples/decoding/plot_haxby_different_estimators.html in your browser. Read the code and identify"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
